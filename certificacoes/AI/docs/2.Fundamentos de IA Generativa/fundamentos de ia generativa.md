# üöÄ Fundamentos de IA Generativa

*Dom√≠nio 2 - Representa 24% da nota*

Para o Dom√≠nio 2 (Fundamentos de IA Generativa), a AWS quer testar se voc√™ entende a "ci√™ncia" por tr√°s da cria√ß√£o de conte√∫do e como os modelos de base funcionam antes de serem aplicados.

---

## ü§ñ O que √© IA Generativa (GenAI)?

Diferente da IA Preditiva (que classifica ou prev√™ n√∫meros), a GenAI cria **novos dados** (texto, imagem, √°udio, c√≥digo) baseando-se em padr√µes aprendidos.

### Modelos de Base (Foundation Models - FMs)

S√£o modelos treinados em uma escala massiva de dados (terabytes de internet/livros) que podem ser adaptados para v√°rias tarefas sem precisar de treino do zero.

### Capacidades Principais

| Tipo | Fun√ß√£o | Exemplos |
| :--- | :--- | :--- |
| **Text-to-Text** | Resumos, tradu√ß√£o, chat | Claude, Llama |
| **Text-to-Image** | Cria√ß√£o de artes/diagramas | Stable Diffusion, Titan Image Generator |
| **Text-to-Code** | Gerar scripts SQL, Python, etc. | Codex, CodeLlama |

---

## üß† A Arquitetura Transformer (O Motor da GenAI)

> [!NOTE]
> Voc√™ n√£o precisa saber a matem√°tica, mas precisa entender os conceitos.

| Conceito | Defini√ß√£o |
| :--- | :--- |
| **Self-Attention** | Permite que o modelo entenda a rela√ß√£o entre palavras distantes em uma frase. Ex: Em "O animal n√£o atravessou a rua porque ele estava cansado", a aten√ß√£o liga "ele" a "animal" |
| **Tokens** | A "moeda" da GenAI. Palavras s√£o quebradas em peda√ßos |
| **Janela de Contexto** | O limite de "mem√≥ria" de curto prazo do modelo em uma conversa |

> [!TIP]
> **Regra de bolso:** 1.000 tokens ‚âà 750 palavras.

---

## ‚öôÔ∏è Par√¢metros de Infer√™ncia (Ajustando a Resposta)

> [!IMPORTANT]
> No console do Amazon Bedrock, voc√™ ver√° esses controles. Eles caem direto na prova:

| Par√¢metro | O que faz? | Quando usar? |
| :--- | :--- | :--- |
| **Temperature** | Controla a aleatoriedade/criatividade | 0.0: Respostas exatas (TI/C√≥digo). 1.0: Criatividade total |
| **Top-P / Top-K** | Filtra as palavras mais prov√°veis | Usado para evitar que o modelo escolha palavras sem sentido |
| **Stop Sequences** | Diz ao modelo onde parar de escrever | Ex: Parar de gerar texto quando encontrar um "###" |

---

## üîÑ O Ciclo de Vida do Modelo de Base

| Fase | Descri√ß√£o |
| :--- | :--- |
| **Pre-training** | O modelo aprende gram√°tica e fatos do mundo (Custo alt√≠ssimo) |
| **Instruction Fine-Tuning** | O modelo aprende a seguir comandos (ex: "Traduza este texto") |
| **RLHF** | Humanos d√£o notas √†s respostas. Isso alinha o modelo com valores humanos e seguran√ßa |

---

## üìä Avalia√ß√£o de Modelos (Model Evaluation)

Como saber qual modelo usar? A AWS cobra as m√©tricas e o processo:

| Tipo | M√©trica | Descri√ß√£o |
| :--- | :--- | :--- |
| **Autom√°tica** | ROUGE / BLEU | Usadas para tradu√ß√£o e resumos (comparam a resposta da IA com um texto de refer√™ncia) |
| **Humana** | Revis√£o manual | No Amazon Bedrock, voc√™ pode configurar um fluxo onde pessoas revisam as respostas para crit√©rios de toxicidade, precis√£o e estilo |

---

## ‚ö†Ô∏è Desafios e Riscos (Fundamentais para IA Respons√°vel)

| Risco | Descri√ß√£o |
| :--- | :--- |
| **Alucina√ß√µes** | O modelo gera informa√ß√µes falsas com confian√ßa |
| **Vazamento de Dados** | Risco de dados sens√≠veis do treino aparecerem nas respostas |
| **Custo** | GenAI √© computacionalmente cara; entender o custo por token √© vital |

---

## üìù Check-list de Termos para Decorar

- [ ] **Zero-shot Prompting:** Pedir algo sem dar exemplos
- [ ] **Few-shot Prompting:** Dar 2 ou 3 exemplos no prompt antes da pergunta
- [ ] **Chain-of-Thought:** Pedir para o modelo "pensar passo a passo"
- [ ] **Negative Prompt:** (Em imagens) Dizer o que voc√™ n√£o quer que apare√ßa

---

*Documenta√ß√£o atualizada em: 2026*
